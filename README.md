# Hand Sign Detection Project ðŸ–ï¸

A computer vision project for detecting hands and recognizing hand gestures using YOLO.

## Project Overview

This project implements a two-phase approach:
1. **Phase 1**: Hand detection - Detect hands in images
2. **Phase 2**: Gesture recognition - Classify specific hand gestures (ðŸ‘Œ, ðŸ‘, âœŒï¸, etc.)

## Quick Start

### 1. Collect Training Data

```bash
# Start interactive data collection
python collect_data.py

# Check collected data statistics
python collect_data.py --stats
```

Collect 50-100 images per gesture with varied:
- Hand positions and angles
- Distances from camera
- Lighting conditions
- Backgrounds

### 2. Train Models

```bash
# Phase 1: Train hand detector
python train_hand_detector.py hand --epochs 30

# Phase 2: Train gesture classifier (after hand detector is ready)
python train_hand_detector.py gesture --epochs 50

# Or train both phases
python train_hand_detector.py both
```

### 3. Deploy to Hugging Face

The project includes a ready-to-deploy Gradio app for Hugging Face Spaces:

1. Create a new Space on [Hugging Face](https://huggingface.co/spaces)
2. Upload your trained models to `models/` directory
3. Copy `deployment/huggingface/` contents to your Space
4. The app will automatically load your models

## Project Structure

```
hand-sign-detection/
â”œâ”€â”€ collect_data.py           # Webcam data collection tool
â”œâ”€â”€ train_hand_detector.py    # Training pipeline
â”œâ”€â”€ models/                   # Trained models (auto-created)
â”‚   â”œâ”€â”€ hand_detector_v1.pt
â”‚   â””â”€â”€ gesture_classifier_v1.pt
â”œâ”€â”€ data/                     # Training data
â”‚   â””â”€â”€ raw/                  # Collected images organized by gesture
â”‚       â”œâ”€â”€ ok/
â”‚       â”œâ”€â”€ thumbs_up/
â”‚       â”œâ”€â”€ peace/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ deployment/
â”‚   â””â”€â”€ huggingface/         # Hugging Face deployment
â”‚       â”œâ”€â”€ app.py           # Gradio interface
â”‚       â””â”€â”€ requirements.txt
â””â”€â”€ claude.scratchpad.md     # Experiment tracking
```

## Supported Gestures

- ðŸ‘Œ OK sign (`ok`)
- ðŸ‘ Thumbs up (`thumbs_up`)
- âœŒï¸ Peace sign (`peace`)
- âœŠ Fist (`fist`)
- ðŸ‘‰ Pointing (`point`)
- ðŸ¤˜ Rock sign (`rock`)
- ðŸ‘‹ Wave (`wave`)
- âœ‹ Stop (`stop`)
- ðŸ–ï¸ Open hand (`hand`)
- Background/No hand (`none`)

## Requirements

```bash
pip install -r requirements.txt
```

Main dependencies:
- `ultralytics` - YOLO implementation
- `opencv-python` - Image processing
- `gradio` - Web interface
- `torch` - Deep learning framework

## Web Interface

Once deployed to Hugging Face, your model will have:
- Live webcam input
- Image upload
- Real-time hand detection with bounding boxes
- Gesture classification with confidence scores
- Interactive demo interface

## Tips for Best Results

1. **Data Quality**: More diverse data > more epochs
2. **Balanced Dataset**: Collect similar amounts for each gesture
3. **Include Negatives**: Collect "none" class (no hands) to reduce false positives
4. **Test Incrementally**: Train for few epochs first to validate approach
5. **Monitor Training**: Watch for overfitting (val loss increasing)

## Next Steps

- [ ] Add more gesture types
- [ ] Implement real-time video processing
- [ ] Add hand tracking (not just detection)
- [ ] Create mobile app version
- [ ] Add gesture sequence recognition

## License

MIT